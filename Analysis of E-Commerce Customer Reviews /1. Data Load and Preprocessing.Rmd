---
title: "Data Load & Preprocessing"
author: "YLG"
date: "`r Sys.Date()`"
output:
  html_document:
    csl: harvard-university-of-bath.csl
---

```{r setup, message=FALSE, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  comment = NA,
  fig.pos = 'H'
)

library(jsonlite)
library(readr)
library(dplyr)
library(cld3)
library(stringr)
library(xml2)
library(purrr)
library(stringr)
```

## Data Load

Read and parse JSON files with `Grocery_and_Gourmet_Food` reviews from a large-scale Amazon Reviews dataset, collected in 2023 by McAuley Lab (@hou2024bridging), and it includes rich features such as:

1. User Reviews (ratings, text, helpfulness votes, etc.);
2. Item Metadata (descriptions, price, raw image, etc.) 

The data was downloaded from https://amazon-reviews-2023.github.io on May 3,2025.

### Read and parse JSON files with reviews

```{r }
# Read and parse JSON reviews
review_lines <- readLines("Grocery_and_Gourmet_Food.jsonl", n = 100000)
review_parsed <- lapply(review_lines, fromJSON, flatten = TRUE)
reviews_raw <- do.call(rbind, review_parsed)  |> 
    as.data.frame()

dplyr::glimpse(reviews_raw)
```


```{r eval = TRUE}
# Read and parse JSON metadata

# Define the fields needed
selected_fields <- c("main_category", "title", "store", "parent_asin")

# Read and extract only selected fields
metadata <- readLines("meta_Grocery_and_Gourmet_Food.jsonl") |>
    map(\(x) tryCatch(fromJSON(x, flatten = TRUE), error = \(e) NULL)) |>
    compact() |>
    map(\(x) {
        x[names(x) %in% selected_fields]  # keep only selected fields
    }) |>
    bind_rows()

readr::write_csv(metadata, "metadata_filtered.csv") # writes as UTF-8 by default
```

### Preprocessing steps for reviews

1. Remove duplicates
2. Keep verified purchases only
3. Remove irrelevant fields, such as `images`, `user_id`, `timestamp`, `helpful_vote`, `verified_purchase`, `asin`
4. Keep only reviews in English
5. Clean noise like excessive whitespace or HTML
6. Convert `rating` to numeric values
7. Convert `parent_asin` to character

```{r}
# function to remove HTML tags
clean_html <- function(x) {
    map_chr(x, ~ xml_text(read_html(paste0("<body>", .x, "</body>")))) |> 
        str_squish()
}

reviews <- reviews_raw |> dplyr::distinct() |>
    dplyr::filter(verified_purchase == "TRUE") |> 
               select(-images, -user_id, -timestamp, -helpful_vote, -verified_purchase, -asin) |> filter(detect_language(as.character(text)) == 'en' & detect_language(as.character(title)) == 'en') |>
    mutate(
        text = text |>
            clean_html() |>
            str_squish(),
        title = title |>
            clean_html() |>
            str_squish(),
        rating = as.numeric(rating),
        parent_asin = as.character(parent_asin)
    )
```

## Join Reviews and Metadata

Further preprocessing:

1. Convert `parent_asin`, `main_category`, `title.y`, `store` to character

2. Rename `title.x` to `review_title`, `text` to `review_body`, `title.y` to `product_title`, `store` to `retailer` for better readability.

3. Merge the review title with the review body, as titles often carry strong sentiments or summarise the review. While Transformers work best with rich, natural language, longer and context-rich inputs usually help models like BERT or RoBERTa form a more accurate representation. While merging, we consider duplicates when users repeat the text from the title in the body of the review.

```{r}
# Join reviews and metadata

metadata <- readr::read_csv("metadata_filtered.csv")

df <- reviews |> 
    left_join(metadata, by = "parent_asin") 
```

```{r}
df <- df |>   
    mutate(
        parent_asin = as.character(parent_asin),
        main_category = as.character(main_category),
        title.y = as.character(title.y),
        store = as.character(store)
      #   review = if_else(
      # str_detect(title.x, "[.!?]$"),
      # str_c(title.x, text, sep = " "),
      # str_c(title.x, ". ", text))
    ) |>
    mutate(
    title_clean = str_trim(title.x),
    body_clean = str_trim(text),
    is_duplicate = str_to_lower(title_clean) == str_to_lower(body_clean),
    review = case_when(
      is_duplicate ~ body_clean,
      str_detect(title_clean, "[.!?]$") ~ str_c(title_clean, body_clean, sep = " "),
      TRUE ~ str_c(title_clean, ". ", body_clean)
    )
  ) |>
  select(-title_clean, -body_clean, -is_duplicate) |>
    rename(
        review_title = title.x,          # rename title.x to review_title
        review_body = text,              # rename text to review_text
        product_title = title.y,         # rename title.y to product_title
        retailer = store,                 # rename store to retailer
    ) |> 
    filter(retailer != 'NULL') |>
    filter(main_category != 'NULL') |>
    mutate(review = str_replace_all(review, "_", ""))

dplyr::glimpse(df)
```


## Calculate Word and Sentence Counts

Counting sentences along with words during EDA of Amazon reviews helps assess not just the length but also the structure and clarity of the writing. It allows calculation of average sentence length, which can reflect tone, readability, or emotional intensity. This additional layer of analysis can help identify outliers, improve feature engineering, and support tasks like sentiment or quality prediction.

```{r}
library(tidytext)
df <- df |>
  mutate(temp_id = row_number()) |>
  left_join(
    df |>
      mutate(temp_id = row_number()) |>
      unnest_tokens(word, review) |>
      count(temp_id, name = "word_count"),
    by = "temp_id"
  ) |>
  left_join(
    df |>
      mutate(temp_id = row_number()) |>
      unnest_tokens(sentence, review, token = "sentences") |>
      count(temp_id, name = "sentence_count"),
    by = "temp_id"
  ) |>
    filter(word_count > 10) |>
  select(-temp_id, -review_title, -review_body)


```


### Stratified Sampling

A stratified sampling strategy will ensure a balanced representation across star ratings.

```{r}
set.seed(123)  # for reproducibility

df_sampled <- df |>
    group_by(rating) |>
    slice_sample(n = 500) |>  # up to 2000
    ungroup() |>
  rename_with(~ .x |>
                str_replace_all("_", " ") |>
                str_to_title())
```

Check if there are any other anomalies to be addressed.

```{r}
# Summary of detected anomalies in the dataset
anomalies <- xray::anomalies(df_sampled)

knitr::kable(
  anomalies$variables, 
  caption = "Summary of Detected Anomalies in the Stratified Sample")
```

Save the final output for further processing by normalising the encoding to ensure UTF-8 (to avoid errors in tokenisation).

```{r}
readr::write_csv(df_sampled, "reviews_clean.csv")
```
