glance(summary(training_data)) |>
kable(
caption = "Summary of the quarterly unemployment in Canada in 1962-1973",
digits = 2) |>
kable_styling(
position = "center",
full_width = FALSE)
# Regression Model
regression_model <- tslm(training_data ~ trend + I(trend^2) + I(trend^3)+ season)
{ cat("Regression model summary\n"); print(summary(regression_model)) }
# Test for normality
glance(shapiro.test(regression_model$residuals)) |>
kable(
caption = "Shapiro-Wilk test of the regression model's training residuals",
digits = 2) |>
kable_styling(
position = "center",
full_width = FALSE)
# Plot histogram of residuals
{ cat("Training residuals from the regression model\n"); checkresiduals(regression_model) }
# Perform Ljung-Box test on the residuals
ljung_box_test <- Box.test(regression_model$residuals, lag = 1, type = "Ljung-Box")
# Print the results
glance((ljung_box_test)) |>
kable(
caption = "Box-Ljung test of the regression model's training residuals",
digits = 2) |>
kable_styling(
position = "center",
full_width = FALSE)
# Calculate residuals
residuals_regression <- forecast_regression$mean - test_data
checkresiduals(residuals_regression)
# Plot histogram of residuals
checkresiduals(regression_model)
# Plot histogram of residuals
checkresiduals(hw_model)
# Test for normality
glance(shapiro.test(hw_model$residuals)) |>
kable(
caption = "Shapiro-Wilk test of the exponential smoothing model's training residuals",
digits = 2) |>
kable_styling(
position = "center",
full_width = FALSE)
# Calculate residuals
residuals_hw <- forecast_hw$mean - test_data
checkresiduals(residuals_hw)
# Plot histogram of residuals
checkresiduals(arima_model)
# Test for normality
{cat("Shapiro-Wilk test of the ARIMA model’s training residuals\n"); shapiro.test(arima_model$residuals)}
# Calculate residuals
residuals_arima <- forecast_arima$mean - test_data
checkresiduals(residuals_arima)
# Calculate evaluation metrics
mae_arima <- mean(abs(residuals_arima))
rmse_arima <- sqrt(mean(residuals_arima^2))
mape_arima <- mean(abs(residuals_arima / test_data)) * 100
bias_arima <- mean(residuals_arima)
# Create a data frame for the metrics
evaluation_arima <- data.frame(
Metric = c("Mean Absolute Error (MAE)", "Root Mean Squared Error (RMSE)",
"Mean Absolute Percentage Error (MAPE)", "Forecast Bias"),
Value = c(mae_arima, rmse_arima, mape_arima, bias_arima)
)
# Remove column names
colnames(evaluation_arima) <- NULL
# Print the data frame
evaluation_arima |> kable(
caption = "Error measures evaluating the ARIMA model’s out-of-sample accuracy",
digits = 2) |>
kable_styling(
position = "center",
full_width = FALSE)
# Store evaluation metrics for each model in a data frame
evaluation_metrics <- data.frame(
Model = c("ARIMA", "Exponential Smoothing", "Regression"),
MAE = c(mae_arima, mae_hw, mae_regression),
RMSE = c(rmse_arima, rmse_hw, rmse_regression),
MAPE = c(mape_arima, mape_hw, mape_regression),
Bias = c(bias_arima, bias_hw, bias_regression)
)
# Print the evaluation metrics for comparison
evaluation_metrics |>
kable(
caption = "Error measures evaluating out-of-sample accuracy of the three models",
digits = 2) |>
kable_styling(
position = "center",
full_width = FALSE)
# Select the model with the lowest values for the evaluation metrics
best_model <- evaluation_metrics[which.min(evaluation_metrics$MAPE), ]
# Print the best model
cat("Best Model based on MAPE:", best_model$Model, "\n")
{cat("M3 competition series ID 1001\n"); print(M3[[1001]])}
# Calculate average MAPE and sMAPE for each method
avg_mape_ets <- mean(mape_ets, na.rm = TRUE)
avg_smape_ets <- mean(smape_ets, na.rm = TRUE)
avg_mape_theta <- mean(mape_theta, na.rm = TRUE)
avg_smape_theta <- mean(smape_theta, na.rm = TRUE)
avg_mape_damped <- mean(mape_damped, na.rm = TRUE)
avg_smape_damped <- mean(smape_damped, na.rm = TRUE)
# Store evaluation metrics for each model in a data frame
ets_batch_evaluation_metrics <- data.frame(
Model = c("ETS", "Theta", "Damped Exponential Smoothing"),
MAPE = c(avg_mape_ets, avg_mape_theta, avg_mape_damped),
sMAPE = c(avg_smape_ets, avg_smape_theta, avg_smape_damped)
)
# Print the evaluation metrics for comparison
ets_batch_evaluation_metrics |>
kable(
caption = "Error measures evaluating automatic ETS model's out-of-sample accuracy",
digits = 2) |>
kable_styling(
position = "center",
full_width = FALSE)
# Select the model with the lowest values for MAPE
ets_batch_best_model_mape <- ets_batch_evaluation_metrics[which.min(ets_batch_evaluation_metrics$MAPE), ]
# Select the model with the lowest values for sMAPE
ets_batch_best_model_smape <- ets_batch_evaluation_metrics[which.min(ets_batch_evaluation_metrics$sMAPE), ]
# Print the best model
cat("Best model based on MAPE:", ets_batch_best_model_mape$Model, "\n")
cat("Best model based on sMAPE:", ets_batch_best_model_smape$Model, "\n")
# Calculate average MAPE and sMAPE for each method
avg_mape_tbats <- mean(mape_tbats, na.rm = TRUE)
avg_smape_tbats <- mean(smape_tbats, na.rm = TRUE)
avg_mape_theta <- mean(mape_theta, na.rm = TRUE)
avg_smape_theta <- mean(smape_theta, na.rm = TRUE)
avg_mape_damped <- mean(mape_damped, na.rm = TRUE)
avg_smape_damped <- mean(smape_damped, na.rm = TRUE)
# Store evaluation metrics for each model in a data frame
tbats_batch_evaluation_metrics <- data.frame(
Model = c("TBATS", "Theta", "Damped Exponential Smoothing"),
MAPE = c(avg_mape_tbats, avg_mape_theta, avg_mape_damped),
sMAPE = c(avg_smape_tbats, avg_smape_theta, avg_smape_damped)
)
# Print the evaluation metrics for comparison
tbats_batch_evaluation_metrics |>
kable(
caption = "Error measures evaluating automatic TBATS model's out-of-sample accuracy",
digits = 2) |>
kable_styling(
position = "center",
full_width = FALSE)
# Select the model with the lowest values for MAPE
tbats_batch_best_model_mape <- tbats_batch_evaluation_metrics[which.min(tbats_batch_evaluation_metrics$MAPE), ]
# Select the model with the lowest values for sMAPE
tbats_batch_best_model_smape <- tbats_batch_evaluation_metrics[which.min(tbats_batch_evaluation_metrics$sMAPE), ]
# Print the best model
cat("Best model based on MAPE:", tbats_batch_best_model_mape$Model, "\n")
cat("Best model based on sMAPE:", tbats_batch_best_model_smape$Model, "\n")
# Store evaluation metrics for each method in a data frame
evaluation_metrics_summary <- data.frame(
Method = c("ARIMA", "ETS", "TBATS", "Theta", "Damped Exponential Smoothing"),
MAPE = c(avg_mape_arima, avg_mape_ets, avg_mape_tbats, avg_mape_theta, avg_mape_damped),
sMAPE = c(avg_smape_arima, avg_smape_ets, avg_smape_tbats, avg_smape_theta, avg_smape_damped)
)
# Sort the data frame in ascending order based on both MAPE and sMAPE values
sorted_metrics <- evaluation_metrics_summary[order(evaluation_metrics_summary$MAPE, evaluation_metrics_summary$sMAPE), ]
# Print the sorted data frame
sorted_metrics |>
kable(
caption = "Error measures evaluating out-of-sample accuracy of the automatic models",
digits = 2) |>
kable_styling(
position = "center",
full_width = FALSE)
# Identify the row corresponding to the best model
best_model_row <- sorted_metrics[1, 1]
# Print the best model with highlighting
cat("Best model based on MAPE and sMAPE:", best_model_row)
knitr::opts_chunk$set(comment = NA, echo = TRUE, message=FALSE, warning=FALSE, fig.pos = 'H')
# Load required libraries
library(forecast)
library(Mcomp)
library(ggplot2)
library(fpp3)
library(fable)
library(dplyr)
library(tibble)
library("plotly")
library("RColorBrewer")
library("ggthemes")
library("kableExtra")
library("knitr")
library("broom")
library("scales")
kable(summary(training_data), caption = "Summary of the quarterly unemployment in Canada in 1962-1973")
glance(summary(training_data)) |>
kable(
caption = "Summary of the quarterly unemployment in Canada in 1962-1973",
digits = 2) |>
kable_styling(
position = "center",
full_width = FALSE)
kable(summary(glance(summary(training_data)), caption = "Summary of the quarterly unemployment in Canada in 1962-1973")
glance(summary(training_data)) |>
kable(
caption = "Summary of the quarterly unemployment in Canada in 1962-1973",
digits = 2) |>
kable_styling(
position = "center",
full_width = FALSE)
kable(glance(summary(training_data)), caption = "Summary of the quarterly unemployment in Canada in 1962-1973")
glance(summary(training_data)) |>
kable(
caption = "Summary of the quarterly unemployment in Canada in 1962-1973",
digits = 2) |>
kable_styling(
position = "center",
full_width = FALSE)
kable(glance(summary(training_data)), digits = 2, caption = "Summary of the quarterly unemployment in Canada in 1962-1973")|>
kable_styling(
position = "center",
full_width = FALSE
)
kable(glance(summary(training_data)), digits = 2, caption = "Summary of the quarterly unemployment in Canada in 1962-1973")|>
kable_styling(
position = "center",
full_width = FALSE
)
# Test for normality
kable(glance(glance(shapiro.test(regression_model$residuals))), digits = 2, caption = "Shapiro-Wilk test of the regression model's training residuals")|>
kable_styling(
position = "center",
full_width = FALSE
)
# Test for normality
kable(glance(shapiro.test(regression_model$residuals))), digits = 2, caption = "Shapiro-Wilk test of the regression model's training residuals")|>
kable_styling(
position = "center",
full_width = FALSE
)
# Test for normality
kable(glance(shapiro.test(regression_model$residuals)), digits = 2, caption = "Shapiro-Wilk test of the regression model's training residuals")|>
kable_styling(
position = "center",
full_width = FALSE
)
# Perform Ljung-Box test on the residuals
ljung_box_test <- Box.test(regression_model$residuals, lag = 1, type = "Ljung-Box")
kable(glance((ljung_box_test)), digits = 2, caption = "Box-Ljung test of the regression model's training residuals")|>
kable_styling(
position = "center",
full_width = FALSE
)
# Calculate evaluation metrics
mae_regression <- mean(abs(residuals_regression))
rmse_regression <- sqrt(mean(residuals_regression^2))
mape_regression <- mean(abs(residuals_regression / test_data)) * 100
bias_regression <- mean(residuals_regression)
# Create a data frame for the metrics
evaluation_regression <- data.frame(
Metric = c("Mean Absolute Error (MAE)", "Root Mean Squared Error (RMSE)",
"Mean Absolute Percentage Error (MAPE)", "Forecast Bias"),
Value = c(mae_regression, rmse_regression, mape_regression, bias_regression)
)
# Remove column names
colnames(evaluation_regression) <- NULL
# Print the data frame
kable(evaluation_regression, digits = 2, caption = "Error measures evaluating the regression model's out-of-sample accuracy")|>
kable_styling(
position = "center",
full_width = FALSE
)
# Test for normality
kable(glance(shapiro.test(hw_model$residuals)), digits = 2, caption = "Shapiro-Wilk test of the exponential smoothing model's training residuals")|>
kable_styling(
position = "center",
full_width = FALSE
)
# Calculate evaluation metrics
mae_hw <- mean(abs(residuals_hw))
rmse_hw <- sqrt(mean(residuals_hw^2))
mape_hw <- mean(abs(residuals_hw / test_data)) * 100
bias_hw <- mean(residuals_hw)
# Create a data frame for the metrics
evaluation_hw <- data.frame(
Metric = c("Mean Absolute Error (MAE)", "Root Mean Squared Error (RMSE)",
"Mean Absolute Percentage Error (MAPE)", "Forecast Bias"),
Value = c(mae_hw, rmse_hw, mape_hw, bias_hw)
)
# Remove column names
colnames(evaluation_hw) <- NULL
# Print the data frame
kable(evaluation_hw, digits = 2, caption = "Error measures evaluating ETS(M,Ad,M) model's out-of-sample accuracy")|>
kable_styling(
position = "center",
full_width = FALSE
)
library(tseries)
print(adf.test(training_data, alternative = "stationary"))
library(urca)
print(kpss.test(training_data))
print(adf.test(training_diff, alternative = "stationary"))
print(kpss.test(training_diff))
arima_model <- Arima(training_data, order = c(0,1,0), seasonal = c(1,1,0))
print(summary(arima_model))
# Calculate evaluation metrics
mae_arima <- mean(abs(residuals_arima))
rmse_arima <- sqrt(mean(residuals_arima^2))
mape_arima <- mean(abs(residuals_arima / test_data)) * 100
bias_arima <- mean(residuals_arima)
# Create a data frame for the metrics
evaluation_arima <- data.frame(
Metric = c("Mean Absolute Error (MAE)", "Root Mean Squared Error (RMSE)",
"Mean Absolute Percentage Error (MAPE)", "Forecast Bias"),
Value = c(mae_arima, rmse_arima, mape_arima, bias_arima)
)
# Remove column names
colnames(evaluation_arima) <- NULL
# Print the data frame
kable(evaluation_arima, digits = 2, caption = "Error measures evaluating the ARIMA model’s out-of-sample accuracy")|>
kable_styling(
position = "center",
full_width = FALSE
)
knitr::opts_chunk$set(comment = NA, echo = TRUE, message=FALSE, warning=FALSE, fig.pos = 'H', knitr.table.caption = FALSE)
# Load required libraries
library(forecast)
library(Mcomp)
library(ggplot2)
library(fpp3)
library(fable)
library(dplyr)
library(tibble)
library("plotly")
library("RColorBrewer")
library("ggthemes")
library("kableExtra")
library("knitr")
library("broom")
library("scales")
{cat("Number of first differencings: ", ndiffs(training_data), "\n"); cat("Number of seasonal differencings: ", nsdiffs(training_data))}
# Test for normality
print(shapiro.test(arima_model$residuals))
# Calculate average MAPE and sMAPE for each method
avg_mape_arima <- mean(mape_arima, na.rm = TRUE)
avg_smape_arima <- mean(smape_arima, na.rm = TRUE)
avg_mape_theta <- mean(mape_theta, na.rm = TRUE)
avg_smape_theta <- mean(smape_theta, na.rm = TRUE)
avg_mape_damped <- mean(mape_damped, na.rm = TRUE)
avg_smape_damped <- mean(smape_damped, na.rm = TRUE)
# Store evaluation metrics for each model in a data frame
arima_batch_evaluation_metrics <- data.frame(
Model = c("ARIMA", "Theta", "Damped Exponential Smoothing"),
MAPE = c(avg_mape_arima, avg_mape_theta, avg_mape_damped),
sMAPE = c(avg_smape_arima, avg_smape_theta, avg_smape_damped)
)
# Print the evaluation metrics for comparison
kable(arima_batch_evaluation_metrics, digits = 2, caption = "Error measures evaluating automatic ARIMA model's out-of-sample accuracy")|>
kable_styling(
position = "center",
full_width = FALSE
)
# Select the model with the lowest values for MAPE
arima_batch_best_model_mape <- arima_batch_evaluation_metrics[which.min(arima_batch_evaluation_metrics$MAPE), ]
# Select the model with the lowest values for sMAPE
arima_batch_best_model_smape <- arima_batch_evaluation_metrics[which.min(arima_batch_evaluation_metrics$sMAPE), ]
# Print the best model
{cat("Best model based on MAPE:", arima_batch_best_model_mape$Model, "\n"); cat("Best model based on sMAPE:", arima_batch_best_model_smape$Model, "\n")}
# Calculate average MAPE and sMAPE for each method
avg_mape_ets <- mean(mape_ets, na.rm = TRUE)
avg_smape_ets <- mean(smape_ets, na.rm = TRUE)
avg_mape_theta <- mean(mape_theta, na.rm = TRUE)
avg_smape_theta <- mean(smape_theta, na.rm = TRUE)
avg_mape_damped <- mean(mape_damped, na.rm = TRUE)
avg_smape_damped <- mean(smape_damped, na.rm = TRUE)
# Store evaluation metrics for each model in a data frame
ets_batch_evaluation_metrics <- data.frame(
Model = c("ETS", "Theta", "Damped Exponential Smoothing"),
MAPE = c(avg_mape_ets, avg_mape_theta, avg_mape_damped),
sMAPE = c(avg_smape_ets, avg_smape_theta, avg_smape_damped)
)
# Print the evaluation metrics for comparison
kable(ets_batch_evaluation_metrics, digits = 2, caption = "Error measures evaluating automatic ETS model's out-of-sample accuracy")|>
kable_styling(
position = "center",
full_width = FALSE
)
# Select the model with the lowest values for MAPE
ets_batch_best_model_mape <- ets_batch_evaluation_metrics[which.min(ets_batch_evaluation_metrics$MAPE), ]
# Select the model with the lowest values for sMAPE
ets_batch_best_model_smape <- ets_batch_evaluation_metrics[which.min(ets_batch_evaluation_metrics$sMAPE), ]
# Print the best model
{cat("Best model based on MAPE:", ets_batch_best_model_mape$Model, "\n"); cat("Best model based on sMAPE:", ets_batch_best_model_smape$Model, "\n")}
# Calculate average MAPE and sMAPE for each method
avg_mape_tbats <- mean(mape_tbats, na.rm = TRUE)
avg_smape_tbats <- mean(smape_tbats, na.rm = TRUE)
avg_mape_theta <- mean(mape_theta, na.rm = TRUE)
avg_smape_theta <- mean(smape_theta, na.rm = TRUE)
avg_mape_damped <- mean(mape_damped, na.rm = TRUE)
avg_smape_damped <- mean(smape_damped, na.rm = TRUE)
# Store evaluation metrics for each model in a data frame
tbats_batch_evaluation_metrics <- data.frame(
Model = c("TBATS", "Theta", "Damped Exponential Smoothing"),
MAPE = c(avg_mape_tbats, avg_mape_theta, avg_mape_damped),
sMAPE = c(avg_smape_tbats, avg_smape_theta, avg_smape_damped)
)
# Print the evaluation metrics for comparison
kable(tbats_batch_evaluation_metrics, digits = 2, caption = "Error measures evaluating automatic TBATS model's out-of-sample accuracy")|>
kable_styling(
position = "center",
full_width = FALSE
)
# Select the model with the lowest values for MAPE
tbats_batch_best_model_mape <- tbats_batch_evaluation_metrics[which.min(tbats_batch_evaluation_metrics$MAPE), ]
# Select the model with the lowest values for sMAPE
tbats_batch_best_model_smape <- tbats_batch_evaluation_metrics[which.min(tbats_batch_evaluation_metrics$sMAPE), ]
# Print the best model
{cat("Best model based on MAPE:", tbats_batch_best_model_mape$Model, "\n"); cat("Best model based on sMAPE:", tbats_batch_best_model_smape$Model, "\n")}
knitr::opts_chunk$set(comment = NA, echo = TRUE, message=FALSE, warning=FALSE, fig.pos = 'H')
# Load required libraries
library(forecast)
library(Mcomp)
library(ggplot2)
library(fpp3)
library(fable)
library(dplyr)
library(tibble)
library("plotly")
library("RColorBrewer")
library("ggthemes")
library("kableExtra")
library("knitr")
library("broom")
library("scales")
# Test for normality
kable(glance(shapiro.test(regression_model$residuals)), caption = NA, digits = 2, caption = "Shapiro-Wilk test of the regression model's training residuals")|>
kable_styling(
position = "center",
full_width = FALSE
)
knitr::opts_chunk$set(comment = NA, echo = TRUE, message=FALSE, warning=FALSE, fig.pos = 'H')
# Load required libraries
library(forecast)
library(Mcomp)
library(ggplot2)
library(fpp3)
library(fable)
library(dplyr)
library(tibble)
library("plotly")
library("RColorBrewer")
library("ggthemes")
library("kableExtra")
library("knitr")
library("broom")
library("scales")
?kable
kable(glance(summary(training_data)), digits = 2, caption = "Summary of the quarterly unemployment in Canada in 1962-1973")|>
kable_styling(
position = "center",
full_width = FALSE
)
kable(glance(summary(training_data)), caption = NA, digits = 2, caption = "Summary of the quarterly unemployment in Canada in 1962-1973")|>
kable_styling(
position = "center",
full_width = FALSE
)
knittr::kable(glance(summary(training_data)), caption = NA, digits = 2, caption = "Summary of the quarterly unemployment in Canada in 1962-1973")|>
kable_styling(
position = "center",
full_width = FALSE
)
knitr::kable(glance(summary(training_data)), caption = NA, digits = 2, caption = "Summary of the quarterly unemployment in Canada in 1962-1973")|>
kable_styling(
position = "center",
full_width = FALSE
)
kable(glance(summary(training_data)), digits = 2, caption = "Summary of the quarterly unemployment in Canada in 1962-1973")|>
kable_styling(
position = "center",
full_width = FALSE
)
knitr::opts_chunk$set(comment = NA, echo = TRUE, message=FALSE, warning=FALSE, fig.pos = 'H')
# Load required libraries
library(forecast)
library(Mcomp)
library(ggplot2)
library(fpp3)
library(fable)
library(dplyr)
library(tibble)
library("plotly")
library("RColorBrewer")
library("ggthemes")
library("kableExtra")
library("knitr")
library("broom")
library("scales")
# Load the data
frequency <- 4
data <- M3[[1394]]
data <- subset(data, frequency == frequency)
{ cat("M3 competition series ID 1394\n"); print(data) }
# Assign the historical data to a variable for training the model
training_data <- data$x
# Assign the future data points for testing the model's predictions
test_data <- data$xx
kableExtra::kbl(glance(summary(training_data)), digits = 2, caption = "Summary of the quarterly unemployment in Canada in 1962-1973")|>
kable_styling(
position = "center",
full_width = FALSE
)
# Test for normality
kableExtra::kbl(glance(shapiro.test(regression_model$residuals)), digits = 2, caption = "Shapiro-Wilk test of the regression model's training residuals")|>
kable_styling(
position = "center",
full_width = FALSE
)
